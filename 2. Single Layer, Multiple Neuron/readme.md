# Single Layer, Multiple Neuron

<img src="./assets/diagram.png" width="128"/>

## taking our equation as graph
### look as following graphs below. we can map it to the equation y = mx + c. where m is the weight, x is the input and c is the bias

<img src="./assets/graph-equation-1.png" width="300"/>
<img src="./assets/graph-equation-2.png" width="300"/>
<img src="./assets/graph-equation-3.png" width="300"/>

### ReLU and activation point
<img src="./assets/relu.png" width="300"/>